{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41monster/RePIT_NaturalCirculation2D/blob/main/PreTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67731424-370f-4cc9-82fd-461140afe58c",
      "metadata": {
        "id": "67731424-370f-4cc9-82fd-461140afe58c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Input, Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11d4c85d-8749-49f6-ba9d-1b6970ad1510",
      "metadata": {
        "id": "11d4c85d-8749-49f6-ba9d-1b6970ad1510"
      },
      "outputs": [],
      "source": [
        "######################################################################################\n",
        "# The current version (v.1) has a lot of hardcoding, and we are upgrading our codes. #\n",
        "#####################################################################################\n",
        "\n",
        "# Part 1: Training insturction\n",
        "st = 10 # starting time [s]\n",
        "dt = 0.01 # timestep [s]\n",
        "nt = 3 # number of timestep [-]\n",
        "\n",
        "epochs = 5000\n",
        "checkpoint_path =  \"/home/ubuntu/ML_NC/checkpoints/cp.ckpt\" # you should change the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51394f1-ce3b-47a7-a5a1-5caaf324f84e",
      "metadata": {
        "scrolled": true,
        "id": "b51394f1-ce3b-47a7-a5a1-5caaf324f84e"
      },
      "outputs": [],
      "source": [
        "# Part 2: Data pre-processing\n",
        "# OpenFOAM geometry data upload\n",
        "filename_grid = '/home/ubuntu/ML_NC/centres.csv' # you should change the directory\n",
        "Grid = pd.read_csv(filename_grid, header = None)\n",
        "Grid_slice = Grid.drop(0, axis=0).astype(np.float32)\n",
        "Grid_tf = tf.reshape(Grid_slice, shape =[-1,4])\n",
        "Grid_np = Grid_tf.numpy()\n",
        "Ngrid_xnp = np.array(200*Grid_np[:,1:2]+0.51, dtype=np.int64)\n",
        "Ngrid_ynp = np.array(200*Grid_np[:,2:3]+0.51, dtype=np.int64)\n",
        "grid_number = int(40000) #hardcoding\n",
        "grid_number_x = int(200)\n",
        "grid_number_y = int(200)\n",
        "gc = int(39204)\n",
        "\n",
        "# Preprocessing OpenFOAM time series data to train neural net\n",
        "for nti in range(0,nt):\n",
        "   # OpenFOAM time series data upload\n",
        "    tnow = st + dt*nti\n",
        "    tnow = \"{:g}\".format(tnow)\n",
        "    filename_openfoam = '/home/ubuntu/ML_NC/traindata%s.csv'%tnow\n",
        "\n",
        "    openfoam = pd.read_csv(filename_openfoam, header = None)\n",
        "    openfoam_slice = openfoam.drop(0, axis=0).astype(np.float32)\n",
        "    openfoam_tf = tf.reshape(openfoam_slice, shape =[-1,7])\n",
        "    openfoam_np = openfoam_tf.numpy()\n",
        "\n",
        "    alpha_np = openfoam_np[:,1:2] # in this code alpha is temperature\n",
        "    p_np = openfoam_np[:,2:3] # in this code p, ph is not used for ML input\n",
        "    ph_np = openfoam_np[:,3:4]\n",
        "    U_np = openfoam_np[:,4:6]\n",
        "\n",
        "    # domain matrix\n",
        "    alpha_matrix = np.ones([200,200])\n",
        "    p_matrix = np.ones([200,200])\n",
        "    ph_matrix = np.ones([200,200])\n",
        "    ux_matrix = np.ones([200,200])\n",
        "    uy_matrix = np.ones([200,200])\n",
        "\n",
        "    for i in range(0, grid_number):\n",
        "        ngrid_x = Ngrid_xnp[i]\n",
        "        ngrid_y = Ngrid_ynp[i]\n",
        "        alpha_matrix[ngrid_x-1,ngrid_y-1] = alpha_np[i,0]\n",
        "        p_matrix[ngrid_x-1,ngrid_y-1] = p_np[i,0]\n",
        "        ph_matrix[ngrid_x-1,ngrid_y-1] = ph_np[i,0]\n",
        "        ux_matrix[ngrid_x-1,ngrid_y-1] = U_np[i,0]\n",
        "        uy_matrix[ngrid_x-1,ngrid_y-1] = U_np[i,1]\n",
        "\n",
        "    # saving domain matrix\n",
        "    globals()['alpha_matrix_'+str(tnow)] = copy(alpha_matrix)\n",
        "    globals()['p_matrix_'+str(tnow)] = copy(p_matrix)\n",
        "    globals()['ph_matrix_'+str(tnow)] = copy(ph_matrix)\n",
        "    globals()['ux_matrix_'+str(tnow)] = copy(ux_matrix)\n",
        "    globals()['uy_matrix_'+str(tnow)] = copy(uy_matrix)\n",
        "\n",
        "    # ML dataset production (features)\n",
        "    ngrid_x_zp = grid_number_x + 2; ngrid_y_zp = grid_number_y +2\n",
        "\n",
        "    p_ml = np.ones([grid_number, 5]); ph_ml = np.ones([grid_number, 5]); ux_ml = np.ones([grid_number, 5]); uy_ml = np.ones([grid_number, 5]); alpha_ml = np.ones([grid_number, 5])\n",
        "\n",
        "    # zero padding\n",
        "    p_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); ph_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); ux_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); uy_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); alpha_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]) # zero padding\n",
        "    p_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = p_matrix; ph_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = ph_matrix; ux_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = ux_matrix; uy_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = uy_matrix; alpha_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = alpha_matrix\n",
        "\n",
        "    # for ML input format\n",
        "    k = 0\n",
        "    for i in range(2, 200):     # except 1-layer boundary cells\n",
        "        for j in range(2,200):\n",
        "            p_ml[k,0] = p_matrix_zp[i,j]; p_ml[k,1] = p_matrix_zp[i-1,j]; p_ml[k,2] = p_matrix_zp[i+1,j]; p_ml[k,3] = p_matrix_zp[i,j-1]; p_ml[k,4] = p_matrix_zp[i,j+1];\n",
        "            ph_ml[k,0] = ph_matrix_zp[i,j]; ph_ml[k,1] = ph_matrix_zp[i-1,j]; ph_ml[k,2] = ph_matrix_zp[i+1,j]; ph_ml[k,3] = ph_matrix_zp[i,j-1]; ph_ml[k,4] = ph_matrix_zp[i,j+1];\n",
        "            ux_ml[k,0] = ux_matrix_zp[i,j]; ux_ml[k,1] = ux_matrix_zp[i-1,j]; ux_ml[k,2] = ux_matrix_zp[i+1,j]; ux_ml[k,3] = ux_matrix_zp[i,j-1]; ux_ml[k,4] = ux_matrix_zp[i,j+1];\n",
        "            uy_ml[k,0] = uy_matrix_zp[i,j]; uy_ml[k,1] = uy_matrix_zp[i-1,j]; uy_ml[k,2] = uy_matrix_zp[i+1,j]; uy_ml[k,3] = uy_matrix_zp[i,j-1]; uy_ml[k,4] = uy_matrix_zp[i,j+1];\n",
        "            alpha_ml[k,0] = alpha_matrix_zp[i,j]; alpha_ml[k,1] = alpha_matrix_zp[i-1,j]; alpha_ml[k,2] = alpha_matrix_zp[i+1,j]; alpha_ml[k,3] = alpha_matrix_zp[i,j-1]; alpha_ml[k,4] = alpha_matrix_zp[i,j+1];\n",
        "            k=k+1\n",
        "\n",
        "    globals()['ml_dataset_'+str(nti)] = np.concatenate((alpha_ml,ux_ml,uy_ml),axis = 1) # 각 time series data 저장\n",
        "\n",
        "# ML dataset production (labels)\n",
        "ml_dataset_inp = globals()['ml_dataset_'+str(0)][0:gc,:]\n",
        "ntd = nt - 1\n",
        "for nti in range(0,ntd):\n",
        "    globals()['ml_dataset_diff_'+str(nti)] = globals()['ml_dataset_'+str(nti+1)][0:gc,:] - globals()['ml_dataset_'+str(nti)][0:gc,:]\n",
        "\n",
        "ml_dataset_diff = globals()['ml_dataset_diff_'+str(0)][0:gc,:]\n",
        "ntdd = ntd - 1\n",
        "for nti in range(0,ntdd):\n",
        "    ml_dataset_diff = np.concatenate((ml_dataset_diff, globals()['ml_dataset_diff_'+str(nti+1)][0:gc,:]), axis = 0)\n",
        "    ml_dataset_inp = np.concatenate((ml_dataset_inp, globals()['ml_dataset_'+str(nti+1)][0:gc,:]), axis = 0)\n",
        "\n",
        "\n",
        "ml_dataset = np.concatenate((ml_dataset_diff[:,0:1],ml_dataset_diff[:,5:6],ml_dataset_diff[:,10:11],ml_dataset_inp),axis = 1)\n",
        "\n",
        "# normalization\n",
        "gct = gc * (nt-1) # gc: grid center\n",
        "dataset = pd.DataFrame(ml_dataset[0:gct,3:18]) # hardcode\n",
        "\n",
        "train_stats = dataset.describe()\n",
        "train_stats = train_stats.transpose()\n",
        "\n",
        "train_labels_a = ml_dataset[0:gct,0:1]\n",
        "train_labels_u = ml_dataset[0:gct,1:2]\n",
        "train_labels_v = ml_dataset[0:gct,2:3]\n",
        "train_labels = np.concatenate((train_labels_a, train_labels_u,train_labels_v),axis=1)\n",
        "\n",
        "labels = pd.DataFrame(train_labels)\n",
        "\n",
        "labels_stats = labels.describe()\n",
        "labels_stats = labels_stats.transpose()\n",
        "\n",
        "# define norm function\n",
        "def norm(x):\n",
        "    return (x-train_stats['mean'])/train_stats['std']\n",
        "def norm_label(x):\n",
        "    return (x-labels_stats['mean'])/labels_stats['std']\n",
        "def denorm_label(x):\n",
        "    return x*labels_stats['std']+labels_stats['mean']\n",
        "\n",
        "normed_train_data = norm(dataset)\n",
        "normed_train_data_re= tf.reshape(normed_train_data, shape=[-1,15])\n",
        "\n",
        "normed_train_labels = norm_label(labels)\n",
        "denorm_test = denorm_label(normed_train_labels)\n",
        "\n",
        "\n",
        "train_labels = tf.reshape(normed_train_labels, shape = [-1,3])\n",
        "train_labels = np.reshape(train_labels, (-1,3))\n",
        "\n",
        "dataset_re = tf.reshape(dataset, shape=[-1,15])\n",
        "\n",
        "# input data (normed, raw)\n",
        "x_sum = np.concatenate((normed_train_data_re, dataset_re[0:gct,0:1], dataset_re[0:gct,5:6], dataset_re[0:gct,10:11]),axis=1)\n",
        "# output data\n",
        "y_sum = train_labels\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x, x_val, y, y_val = train_test_split(x_sum,y_sum,test_size=0.2, random_state = 1004)\n",
        "x_pinn = x_sum  # x_pinn, y_pinn for test\n",
        "y_pinn = y_sum\n",
        "\n",
        "# loss_tracker for monotoring during training\n",
        "loss_tracker = keras.metrics.Mean(name=\"val_loss\")\n",
        "loss_tracker_2 = keras.metrics.Mean(name=\"loss\")\n",
        "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Model build and compile\n",
        "net_ph = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_ph\",\n",
        ")\n",
        "\n",
        "net_p = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_p\",\n",
        ")\n",
        "\n",
        "net_u = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_u\",\n",
        ")\n",
        "\n",
        "net_v = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_v\",\n",
        ")\n",
        "\n",
        "net_a = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_a\",\n",
        ")\n",
        "\n",
        "class FVMN(keras.Model):\n",
        "    def __init__(self, net_a, net_ph,net_u, net_v, Rs, x_val=x_val, y_val=y_val, x_pinn=x_pinn, y_pinn=y_pinn):\n",
        "        super(FVMN, self).__init__()\n",
        "        self.net_a = net_a\n",
        "        #self.net_p = net_p\n",
        "        #self.net_ph = net_ph\n",
        "        self.net_u = net_u\n",
        "        self.net_v = net_v\n",
        "        self.Rs = Rs\n",
        "        self.x_val = x_val\n",
        "        self.y_val = y_val\n",
        "        self.x_pinn = x_pinn\n",
        "        self.y_pinn = y_pinn\n",
        "\n",
        "    def call(self,x):\n",
        "        x = self.net_a(x)\n",
        "        #x = self.net_p(x)\n",
        "        #x = self.net_ph(x)\n",
        "        x = self.net_u(x)\n",
        "        x = self.net_v(x)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x_pre, y = data\n",
        "        # the order of the variables depends on the dataset.\n",
        "        x = x_pre[:,0:15]\n",
        "        x_ex = x_pre[:,15:18]\n",
        "        x_ex = tf.cast(x_ex, dtype='float32')\n",
        "\n",
        "        y_a = y[:,0:1]\n",
        "        #y_p = y[:,1:2]\n",
        "        #y_ph = y[:,1:2]\n",
        "        y_u = y[:,1:2]\n",
        "        y_v = y[:,2:3]\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            #multi grads calculation: persistent=True\n",
        "            y_pred_a = self.net_a(x)\n",
        "            #y_pred_p = self.net_p(x)\n",
        "            #y_pred_ph = self.net_ph(x)\n",
        "            y_pred_u = self.net_u(x)  # Forward pass\n",
        "            y_pred_v = self.net_v(x)\n",
        "\n",
        "            # Calculate loss in batches.\n",
        "            loss_a_mse = tf.reduce_mean(tf.square(y_a-y_pred_a), axis = 0)\n",
        "            #loss_p_mse = tf.reduce_mean(tf.square(y_p-y_pred_p), axis = 0)\n",
        "            #loss_ph_mse = tf.reduce_mean(tf.square(y_ph-y_pred_ph), axis = 0)\n",
        "            loss_u_mse = tf.reduce_mean(tf.square(y_u-y_pred_u), axis = 0)\n",
        "            loss_v_mse = tf.reduce_mean(tf.square(y_v-y_pred_v), axis = 0)\n",
        "\n",
        "            # loss for training\n",
        "            loss_a = loss_a_mse\n",
        "            #loss_p = loss_p_mse\n",
        "            #loss_ph = loss_ph_mse\n",
        "            loss_u = loss_u_mse\n",
        "            loss_v = loss_v_mse\n",
        "\n",
        "            #loss_tot = loss_a + loss_p + loss_ph + loss_u + loss_v\n",
        "            loss_tot = loss_a + loss_u + loss_v\n",
        "\n",
        "            loss_a_pinn = loss_a\n",
        "            #loss_p_pinn = loss_p\n",
        "            #loss_ph_pinn = loss_ph\n",
        "            loss_u_pinn = loss_u\n",
        "            loss_v_pinn = loss_v\n",
        "\n",
        "            #loss_tot_pinn = loss_a_pinn + loss_p_pinn + loss_ph_pinn + loss_u_pinn + loss_v_pinn\n",
        "            loss_tot_pinn = loss_a_pinn + loss_u_pinn + loss_v_pinn\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars_a = self.net_a.trainable_variables\n",
        "        #trainable_vars_p = self.net_p.trainable_variables\n",
        "        #trainable_vars_ph = self.net_ph.trainable_variables\n",
        "        trainable_vars_u = self.net_u.trainable_variables\n",
        "        trainable_vars_v = self.net_v.trainable_variables\n",
        "\n",
        "        gradients_a = tape.gradient(loss_a_pinn, trainable_vars_a)\n",
        "        #gradients_p = tape.gradient(loss_p_pinn, trainable_vars_p)\n",
        "        #gradients_ph = tape.gradient(loss_ph_pinn, trainable_vars_ph)\n",
        "        gradients_u = tape.gradient(loss_u_pinn, trainable_vars_u)\n",
        "        gradients_v = tape.gradient(loss_v_pinn, trainable_vars_v)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients_a, trainable_vars_a))\n",
        "        #self.optimizer.apply_gradients(zip(gradients_p, trainable_vars_p))\n",
        "        #self.optimizer.apply_gradients(zip(gradients_ph, trainable_vars_ph))\n",
        "        self.optimizer.apply_gradients(zip(gradients_u, trainable_vars_u))\n",
        "        self.optimizer.apply_gradients(zip(gradients_v, trainable_vars_v))\n",
        "\n",
        "        #validation loss (default: conservation error 포함 x)\n",
        "        x_val = self.x_val\n",
        "        y_val = self.y_val\n",
        "\n",
        "        x_val_2 = x_val[:,0:15]\n",
        "        y_val_a = y_val[:,0:1]\n",
        "        #y_val_p = y_val[:,1:2]\n",
        "        #y_val_ph = y_val[:,1:2]\n",
        "        y_val_u = y_val[:,1:2]\n",
        "        y_val_v = y_val[:,2:3]\n",
        "\n",
        "        y_val_pred_a = self.net_a(x_val_2)\n",
        "        #y_val_pred_p = self.net_p(x_val_2)\n",
        "        #y_val_pred_ph = self.net_ph(x_val_2)\n",
        "        y_val_pred_u = self.net_u(x_val_2)  # Forward pass\n",
        "        y_val_pred_v = self.net_v(x_val_2)\n",
        "\n",
        "        loss_val_a_mse = tf.reduce_mean(tf.square(y_val_a-y_val_pred_a), axis = 0)\n",
        "        #loss_val_p_mse = tf.reduce_mean(tf.square(y_val_p-y_val_pred_p), axis = 0)\n",
        "        #loss_val_ph_mse = tf.reduce_mean(tf.square(y_val_ph-y_val_pred_ph), axis = 0)\n",
        "        loss_val_u_mse = tf.reduce_mean(tf.square(y_val_u-y_val_pred_u), axis = 0)\n",
        "        loss_val_v_mse = tf.reduce_mean(tf.square(y_val_v-y_val_pred_v), axis = 0)\n",
        "\n",
        "        #val_loss = loss_val_a_mse + loss_val_p_mse + loss_val_ph_mse + loss_val_u_mse + loss_val_v_mse\n",
        "        val_loss = loss_val_a_mse + loss_val_u_mse + loss_val_v_mse\n",
        "\n",
        "        # loss, mse for epochs monitoring\n",
        "        loss_tracker.update_state(val_loss)\n",
        "        #loss_tracker_2.update_state(Rs_tot)\n",
        "        loss_tracker_3.update_state(loss_tot)\n",
        "\n",
        "        return {\"val_loss\": loss_tracker.result(), \"loss_tot\": loss_tracker_2.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker, loss_tracker_3]\n",
        "\n",
        "import os\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                save_weights_only=True, save_best_only=True, monitor = 'val_loss',\n",
        "                                verbose=1)\n",
        "model = FVMN(net_a=net_a, net_ph=net_ph, net_u=net_u,net_v=net_v, Rs =10, x_val=x_val, y_val=y_val, x_pinn = x_pinn,y_pinn = y_pinn )\n",
        "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))"
      ],
      "metadata": {
        "id": "AaLIXk_eO3zS"
      },
      "id": "AaLIXk_eO3zS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3782d5a4-ac33-4484-89e1-3c4d7f5ab70b",
      "metadata": {
        "id": "3782d5a4-ac33-4484-89e1-3c4d7f5ab70b"
      },
      "outputs": [],
      "source": [
        "# Part 4: load weights (for continous learning)\n",
        "model.load_weights(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4ca728-c87c-4587-bfa3-e6ee6e326d33",
      "metadata": {
        "scrolled": true,
        "id": "ef4ca728-c87c-4587-bfa3-e6ee6e326d33"
      },
      "outputs": [],
      "source": [
        "# Part 5: Training\n",
        "history =  model.fit(x, y,batch_size=10000, epochs=epochs, callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ba29de-041b-4124-887b-bbb0edd3e4f5",
      "metadata": {
        "id": "63ba29de-041b-4124-887b-bbb0edd3e4f5"
      },
      "outputs": [],
      "source": [
        "# Part 6: Check training procedure\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist\n",
        "def plot_history(history):\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    plt.figure(figsize=(8,12))\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean square error')\n",
        "    plt.plot(hist['epoch'], hist['val_loss'],\n",
        "        label='Val Error')\n",
        "    plt.plot(hist['epoch'], hist['loss_tot'],\n",
        "        label='Train Error')\n",
        "    plt.ylim(0,0.005)\n",
        "    plt.xlim(0,5000)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "# scatter graph\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "x_pinn_test=x_pinn[:,0:15]\n",
        "y_pinn_test=y_pinn[:,0:1]\n",
        "\n",
        "train_predictions_ux = model.net_a(x_pinn_test)\n",
        "\n",
        "plt.scatter(y_pinn_test, train_predictions_ux)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.grid(True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}