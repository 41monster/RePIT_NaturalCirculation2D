{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/41monster/RePIT_NaturalCirculation2D/blob/main/TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4e358a-f890-4930-90ff-87853d5b1e72",
      "metadata": {
        "id": "0c4e358a-f890-4930-90ff-87853d5b1e72"
      },
      "outputs": [],
      "source": [
        "######################################################################################\n",
        "# The current version (v.1) has a lot of hardcoding, and we are upgrading our codes. #\n",
        "######################################################################################\n",
        "\n",
        "# Please run this code after Prediction.ipynb running (if not, you should import functions in Prediction.ipynb)\n",
        "# Part 1: Transfer Learning insturction\n",
        "st = 19.77 # starting time [s] (hardcoding)\n",
        "dt = 0.01 # timestep [s]\n",
        "nt = 3 # number of timestep [-]\n",
        "\n",
        "model.load_weights(checkpoint_path) #pre-trainied weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dbed0f9-9b7d-444f-bca2-d2d5f499a9d5",
      "metadata": {
        "id": "7dbed0f9-9b7d-444f-bca2-d2d5f499a9d5"
      },
      "outputs": [],
      "source": [
        "# Part 2: Data pre-processing and pre-trained model load\n",
        "# OpenFOAM geometry data upload\n",
        "\n",
        "filename_grid = '/home/ubuntu/ML_NC/centres.csv' # you should change the directory\n",
        "Grid = pd.read_csv(filename_grid, header = None)\n",
        "Grid_slice = Grid.drop(0, axis=0).astype(np.float32)\n",
        "Grid_tf = tf.reshape(Grid_slice, shape =[-1,4])\n",
        "Grid_np = Grid_tf.numpy()\n",
        "Ngrid_xnp = np.array(200*Grid_np[:,1:2]+0.51, dtype=np.int64) #tf 데이터는 for 문에서 특히 느림 (why?)\n",
        "Ngrid_ynp = np.array(200*Grid_np[:,2:3]+0.51, dtype=np.int64)\n",
        "grid_number = int(40000)\n",
        "grid_number_x = int(200)\n",
        "grid_number_y = int(200)\n",
        "gc = int(39204)\n",
        "\n",
        "# Preprocessing OpenFOAM time series data to train neural net\n",
        "for nti in range(0,nt):\n",
        "    # OpenFOAM time series data upload\n",
        "    tnow = st + dt*nti\n",
        "    tnow = \"{:g}\".format(tnow)\n",
        "\n",
        "    filename_openfoam = '/home/ubuntu/ML_NC/20230711_for_ml_29/postdata%s.csv'%tnow # you should change the directory\n",
        "\n",
        "    openfoam = pd.read_csv(filename_openfoam, header = None)\n",
        "    openfoam_slice = openfoam.drop(0, axis=0).astype(np.float32)\n",
        "    openfoam_tf = tf.reshape(openfoam_slice, shape =[-1,4])\n",
        "    openfoam_np = openfoam_tf.numpy()\n",
        "\n",
        "    alpha_np = openfoam_np[:,1:2]\n",
        "    p_np = openfoam_np[:,1:2]\n",
        "    ph_np = openfoam_np[:,1:2]\n",
        "    U_np = openfoam_np[:,2:4]\n",
        "\n",
        "    # domain matrix\n",
        "    alpha_matrix = np.ones([200,200])\n",
        "    p_matrix = np.ones([200,200])\n",
        "    ph_matrix = np.ones([200,200])\n",
        "    ux_matrix = np.ones([200,200])\n",
        "    uy_matrix = np.ones([200,200])\n",
        "\n",
        "    for i in range(0, grid_number):\n",
        "        ngrid_x = Ngrid_xnp[i]\n",
        "        ngrid_y = Ngrid_ynp[i]\n",
        "        alpha_matrix[ngrid_x-1,ngrid_y-1] = alpha_np[i,0]\n",
        "        p_matrix[ngrid_x-1,ngrid_y-1] = p_np[i,0]\n",
        "        ph_matrix[ngrid_x-1,ngrid_y-1] = ph_np[i,0]\n",
        "        ux_matrix[ngrid_x-1,ngrid_y-1] = U_np[i,0]\n",
        "        uy_matrix[ngrid_x-1,ngrid_y-1] = U_np[i,1]\n",
        "\n",
        "    # saving domain matrix\n",
        "    globals()['alpha_matrix_'+str(tnow)] = copy(alpha_matrix)\n",
        "    globals()['p_matrix_'+str(tnow)] = copy(p_matrix)\n",
        "    globals()['ph_matrix_'+str(tnow)] = copy(ph_matrix)\n",
        "    globals()['ux_matrix_'+str(tnow)] = copy(ux_matrix)\n",
        "    globals()['uy_matrix_'+str(tnow)] = copy(uy_matrix)\n",
        "\n",
        "    # ML dataset production (features)\n",
        "    ngrid_x_zp = grid_number_x + 2; ngrid_y_zp = grid_number_y +2\n",
        "\n",
        "    p_ml = np.ones([grid_number, 5]); ph_ml = np.ones([grid_number, 5]); ux_ml = np.ones([grid_number, 5]); uy_ml = np.ones([grid_number, 5]); alpha_ml = np.ones([grid_number, 5])\n",
        "\n",
        "    # zero padding\n",
        "    p_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); ph_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); ux_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); uy_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]); alpha_matrix_zp = np.zeros([ngrid_x_zp,ngrid_y_zp]) # zero padding\n",
        "    p_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = p_matrix; ph_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = ph_matrix; ux_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = ux_matrix; uy_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = uy_matrix; alpha_matrix_zp[1:ngrid_x_zp-1, 1:ngrid_y_zp-1] = alpha_matrix\n",
        "\n",
        "    # for ML input format\n",
        "    k = 0\n",
        "    for i in range(2, 200):    # except 1-layer boundary cells\n",
        "        for j in range(2,200):\n",
        "            p_ml[k,0] = p_matrix_zp[i,j]; p_ml[k,1] = p_matrix_zp[i-1,j]; p_ml[k,2] = p_matrix_zp[i+1,j]; p_ml[k,3] = p_matrix_zp[i,j-1]; p_ml[k,4] = p_matrix_zp[i,j+1];\n",
        "            ph_ml[k,0] = ph_matrix_zp[i,j]; ph_ml[k,1] = ph_matrix_zp[i-1,j]; ph_ml[k,2] = ph_matrix_zp[i+1,j]; ph_ml[k,3] = ph_matrix_zp[i,j-1]; ph_ml[k,4] = ph_matrix_zp[i,j+1];\n",
        "            ux_ml[k,0] = ux_matrix_zp[i,j]; ux_ml[k,1] = ux_matrix_zp[i-1,j]; ux_ml[k,2] = ux_matrix_zp[i+1,j]; ux_ml[k,3] = ux_matrix_zp[i,j-1]; ux_ml[k,4] = ux_matrix_zp[i,j+1];\n",
        "            uy_ml[k,0] = uy_matrix_zp[i,j]; uy_ml[k,1] = uy_matrix_zp[i-1,j]; uy_ml[k,2] = uy_matrix_zp[i+1,j]; uy_ml[k,3] = uy_matrix_zp[i,j-1]; uy_ml[k,4] = uy_matrix_zp[i,j+1];\n",
        "            alpha_ml[k,0] = alpha_matrix_zp[i,j]; alpha_ml[k,1] = alpha_matrix_zp[i-1,j]; alpha_ml[k,2] = alpha_matrix_zp[i+1,j]; alpha_ml[k,3] = alpha_matrix_zp[i,j-1]; alpha_ml[k,4] = alpha_matrix_zp[i,j+1];\n",
        "            k=k+1\n",
        "\n",
        "    globals()['ml_dataset_'+str(nti)] = np.concatenate((alpha_ml,ux_ml,uy_ml),axis = 1) # 각 time series data 저장\n",
        "\n",
        "# ML dataset production (labels)\n",
        "ml_dataset_inp = globals()['ml_dataset_'+str(0)][0:gc,:]\n",
        "ntd = nt - 1\n",
        "for nti in range(0,ntd):\n",
        "    globals()['ml_dataset_diff_'+str(nti)] = globals()['ml_dataset_'+str(nti+1)][0:gc,:] - globals()['ml_dataset_'+str(nti)][0:gc,:]\n",
        "\n",
        "ml_dataset_diff = globals()['ml_dataset_diff_'+str(0)][0:gc,:]\n",
        "ntdd = ntd - 1\n",
        "for nti in range(0,ntdd):\n",
        "    ml_dataset_diff = np.concatenate((ml_dataset_diff, globals()['ml_dataset_diff_'+str(nti+1)][0:gc,:]), axis = 0)\n",
        "    ml_dataset_inp = np.concatenate((ml_dataset_inp, globals()['ml_dataset_'+str(nti+1)][0:gc,:]), axis = 0)\n",
        "\n",
        "\n",
        "ml_dataset = np.concatenate((ml_dataset_diff[:,0:1],ml_dataset_diff[:,5:6],ml_dataset_diff[:,10:11],ml_dataset_inp),axis = 1)\n",
        "\n",
        "# normalization\n",
        "gct = gc * (nt-1) # gc: grid center (default gc = gtot)\n",
        "dataset = pd.DataFrame(ml_dataset[0:gct,3:18])\n",
        "\n",
        "train_stats = dataset.describe()\n",
        "train_stats = train_stats.transpose()\n",
        "\n",
        "train_labels_a = ml_dataset[0:gct,0:1]\n",
        "train_labels_u = ml_dataset[0:gct,1:2]\n",
        "train_labels_v = ml_dataset[0:gct,2:3]\n",
        "train_labels = np.concatenate((train_labels_a, train_labels_u,train_labels_v),axis=1)\n",
        "\n",
        "labels = pd.DataFrame(train_labels)\n",
        "\n",
        "labels_stats = labels.describe()\n",
        "labels_stats = labels_stats.transpose()\n",
        "\n",
        "# norm function already defined in PreTraining.ipynb\n",
        "'''\n",
        "def norm(x):\n",
        "    return (x-train_stats['mean'])/train_stats['std']\n",
        "def norm_label(x):\n",
        "    return (x-labels_stats['mean'])/labels_stats['std']\n",
        "def denorm_label(x):\n",
        "    return x*labels_stats['std']+labels_stats['mean']\n",
        "'''\n",
        "\n",
        "normed_train_data = norm(dataset)\n",
        "normed_train_data_re= tf.reshape(normed_train_data, shape=[-1,15])\n",
        "\n",
        "normed_train_labels = norm_label(labels)\n",
        "denorm_test = denorm_label(normed_train_labels)\n",
        "\n",
        "\n",
        "train_labels = tf.reshape(normed_train_labels, shape = [-1,3])\n",
        "train_labels = np.reshape(train_labels, (-1,3))\n",
        "\n",
        "dataset_re = tf.reshape(dataset, shape=[-1,15])\n",
        "\n",
        "# input data (normed, raw)\n",
        "x_sum = np.concatenate((normed_train_data_re, dataset_re[0:gct,0:1], dataset_re[0:gct,5:6], dataset_re[0:gct,10:11]),axis=1)\n",
        "# output data\n",
        "y_sum = train_labels\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x, x_val, y, y_val = train_test_split(x_sum,y_sum,test_size=0.2, random_state = 1004)\n",
        "x_pinn = x_sum  # x_pinn, y_pinn for test\n",
        "y_pinn = y_sum\n",
        "\n",
        "# loss_tracker for monotoring during training\n",
        "loss_tracker = keras.metrics.Mean(name=\"val_loss\")\n",
        "#loss_tracker_2 = keras.metrics.Mean(name=\"loss\")\n",
        "loss_tracker_3 = keras.metrics.Mean(name=\"loss\")\n",
        "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
        "\n",
        "\n",
        "# pre-trained FVMN model load\n",
        "net_ph = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_ph\",\n",
        ")\n",
        "\n",
        "net_p = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_p\",\n",
        ")\n",
        "\n",
        "net_u = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_u\",\n",
        ")\n",
        "\n",
        "net_v = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_v\",\n",
        ")\n",
        "\n",
        "net_a = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(512, activation='relu', input_shape=(15,)),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ],\n",
        "    name=\"net_a\",\n",
        ")\n",
        "\n",
        "import os\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_transfer_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_transfer_path,\n",
        "                                save_weights_only=True, save_best_only=True, monitor = 'val_loss',\n",
        "                                verbose=1)\n",
        "\n",
        "\n",
        "model = FVMN_PINNs(net_a=net_a, net_ph=net_ph, net_u=net_u,net_v=net_v, Rs =10, x_val=x_val, y_val=y_val, x_pinn = x_pinn,y_pinn = y_pinn )\n",
        "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeef0748-672a-4d7e-9abd-7942003c6dd5",
      "metadata": {
        "id": "eeef0748-672a-4d7e-9abd-7942003c6dd5"
      },
      "outputs": [],
      "source": [
        "# Part 3: pre-trained weighs load\n",
        "model.load_weights(checkpoint_transfer_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae80fa7-1f84-4bc9-b35d-ac2f16511503",
      "metadata": {
        "scrolled": true,
        "id": "aae80fa7-1f84-4bc9-b35d-ac2f16511503"
      },
      "outputs": [],
      "source": [
        "# Part 4: transfer learning (no freeze layer in this code)\n",
        "history =  model.fit(x, y,batch_size=10000, epochs=20, callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae2980d-5d50-418b-a16a-143caeb37df9",
      "metadata": {
        "id": "fae2980d-5d50-418b-a16a-143caeb37df9"
      },
      "outputs": [],
      "source": [
        "# Part 5: Check training procedure\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist\n",
        "\n",
        "def plot_history(history):\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    plt.figure(figsize=(8,12))\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean square error')\n",
        "    plt.plot(hist['epoch'], hist['val_loss'],\n",
        "        label='Val Error')\n",
        "    plt.plot(hist['epoch'], hist['loss_tot'],\n",
        "        label='Train Error')\n",
        "    plt.ylim(0,0.005)\n",
        "    plt.xlim(0,30)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "\n",
        "# scatter graph\n",
        "model.load_weights(checkpoint_transfer_path)\n",
        "\n",
        "x_pinn_test=x_pinn[:,0:15]\n",
        "y_pinn_test=y_pinn[:,0:1]\n",
        "\n",
        "train_predictions_ux = model.net_a(x_pinn_test)\n",
        "\n",
        "plt.scatter(y_pinn_test, train_predictions_ux)\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.grid(True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}